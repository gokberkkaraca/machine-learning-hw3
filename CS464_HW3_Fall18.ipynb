{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>CS 464</center></h1>\n",
    "<h1><center>Introduction Machine Learning</center></h1>\n",
    "<h1><center>Fall 2018</center></h1>\n",
    "<h1><center>Homework 3</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center>Due: Jan 1, 2019 23:59</center></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>This homework contains both written and programming questions about neural networks. You should implement your programming questions on this notebook. Your plots should also be produced in this notebook. You will also write a report (.pdf) which contains your written answers and plots you will produce here. Each programming question has its own cell for your answer. You can implement your code directly in these cells, or you can call required functions which are defined in a different location for the given question.\n",
    "    </li>\n",
    "    <li>\n",
    "        For written questions, your answers have to be in the report as a \".pdf\" file.\n",
    "    </li>\n",
    "    <li>\n",
    "        For questions that you need to plot, your plot results have to be included in both cell output and your \".pdf\" file.\n",
    "    </li>\n",
    "    <li>\n",
    "        It is <b>NOT ALLOWED</b> to use different libraries than given libraries which are defined in the requirements.txt.\n",
    "    </li>\n",
    "    <li>\n",
    "        It is <b>NOT ALLOWED</b> to use a different deep learning framework than PyTorch.\n",
    "    </li>\n",
    "    <li>\n",
    "        While submitting the homework file, please package your report(\".pdf\") and notebook(\".ipynb\") files as a gzipped TAR file or a ZIP file with the name CS464_HW3_Firstname_Lastname. Please do not use any Turkish letters for any of your files including code files and report file. Upload your homework to Moodle.\n",
    "    </li>\n",
    "    <li>\n",
    "        Any violation of these rules may lead to significant grade deduction.\n",
    "    </li>\n",
    "    <li> You can send an email for your questions to <b>can.uner@bilkent.edu.tr</b> address.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anaconda Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Download anaconda from https://www.anaconda.com/download</li>\n",
    "    <li>Follow the instructions provided in https://conda.io/docs/user-guide/install/index.html#regular-installation</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of Virtual Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Create python3.7 virtual environment for your hw3 using follow command from the command line<br>\n",
    "        <i>> conda create -n HW3 python=3.7 anaconda</i></li>\n",
    "    <li>Activate your virtual environment<br>\n",
    "        <i>> source activate HW3</i></li>\n",
    "    <li>To install auxiliary libraries install attached \"requirements.txt\" and run following command in activated \"hw3\" environment<br>\n",
    "        <i>> pip install -r requirements.txt<i></li>\n",
    "     <li>When you create your virtual environment with \"anaconda\" metapackage, jupyter notebook should be installed. Try:<br>\n",
    "         <i>> jupyter notebook</i>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should install PyTorch to your virtual environment which is created for the hw3. Therefore, you should activate your homework virtual environment before to start PyTorch installation.\n",
    "<li>> source activate HW3</li>\n",
    "\n",
    "After you have activated the virtual environment, then use one of the following commands to install pytorch for CPU for your system. See https://pytorch.org/ for help.\n",
    "<ul>\n",
    "<li>For MacOS:<br>\n",
    "    <i>> conda install pytorch torchvision -c pytorch</i>\n",
    "</li>\n",
    "<li>For Linux:<br>\n",
    "    <i>> conda install pytorch-cpu torchvision-cpu -c pytorch</i>\n",
    "</li>\n",
    "<li>For Windows:<br>\n",
    "    <i>> conda install pytorch-cpu torchvision-cpu -c pytorch</i><br>\n",
    "</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 [20 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that you are provided a two hidden layer neural network designed to solve a binary classification problem. This architecture uses ReLU activation function between two hidden layers. You should choose the appropriate activation function of the output layer and loss function for the binary classification problem. Compute the gradients and update the weights only for a single iteration. You DO NOT need to provide any numeric inputs to feed the network to compute gradients. You only need to show derivations for the gradients in backpropagation. Provide a pdf file containing your answer (Hint: you do not need the neuron sizes if you use matrix notation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 [27 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, you will perform multi-class clasification on a gene expression dataset. Specificically, you will implement a neural network with two hidden layers to predict discrete tumor types: BRCA, KIRC, COAD, LUAD and PRAD.\n",
    "Your features will be continuous gene activity (expression) levels. Download the dataset from the following link:\n",
    "<br>\n",
    "https://archive.ics.uci.edu/ml/datasets/gene+expression+cancer+RNA-Seq. \n",
    "See details below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader [4 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important part of such a task is to implement your own data loader. In this homework, we provide a partial loader for you. This loader is going to be based on a base class named \"Dataset\", provided in PyTorch library. You need to complete the code below  to create your custom \"GEDataset\" class which will be able to load your dataset. \n",
    "Implement the functions whose proptotypes are given but the bodies are omitted. Follow the TODO notes below. You have to divide the dataset into three sections which are <b>\"train (50%)\", \"validation (20%)\" and \"test (30%)\"</b>. These non-overlapping splits, which are subsets of GEDataset, should be retrieved using the \"get_dataset\" function. \n",
    "<br>\n",
    "Hint: The dataset is not normalized and your results will heavily depend on your input.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "label_to_index = {'BRCA': 0, 'KIRC': 1, 'COAD': 2, 'LUAD':3, 'PRAD':4}\n",
    "index_to_label = {v:k for k,v in label_to_index.items()}\n",
    "\n",
    "#https://archive.ics.uci.edu/ml/datasets/gene+expression+cancer+RNA-Seq\n",
    "class GEDataset(Dataset):\n",
    "    \n",
    "    # Define constructor for GEDataset class\n",
    "    # HINT: You can pass processed data samples and their ground truth values as parameters \n",
    "    def __init__(self, **kwargs):\n",
    "        self.data = kwargs['data']\n",
    "        self.labels = kwargs['labels']\n",
    "        self.labels = list(map(lambda item: label_to_index[item], self.labels))\n",
    "        \n",
    "    '''This function should return sample count in the dataset'''\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    '''This function should return a single sample and its ground truth value from the dataset corresponding to index parameter '''\n",
    "    def __getitem__(self, index):\n",
    "        _x = self.data[index]\n",
    "        _y = self.labels[index]\n",
    "        return _x, _y\n",
    "\n",
    "\n",
    "def get_dataset(root):\n",
    "    # Read dataset files from \"data/Q1\" folder\n",
    "    data = np.genfromtxt(root + \"data/Q1/\" + \"data.csv\", delimiter=\",\")\n",
    "    data = data[1:, 1:]\n",
    "    labels = pd.read_csv(root + \"data/Q1/\" + \"labels.csv\")\n",
    "    labels = labels.values[:,1]\n",
    "    \n",
    "    # Split dataset samples into the 3 part for training, validation and test\n",
    "    \n",
    "    train_dataset = data[0:int(len(data)*0.5)]\n",
    "    val_dataset = data[int(len(data)*0.5):int(len(data)*0.7)]\n",
    "    test_dataset = data[int(len(data)*0.7):]\n",
    "    \n",
    "    # Normalize datasets\n",
    "    train_dataset = np.apply_along_axis(normalize_data, 0, train_dataset)\n",
    "    val_dataset = np.apply_along_axis(normalize_data, 0, val_dataset)\n",
    "    test_dataset = np.apply_along_axis(normalize_data, 0, test_dataset)\n",
    "    \n",
    "    kwargs = {'data': train_dataset, 'labels': labels[0:int(len(labels)*0.5)]}\n",
    "    train_dataset = GEDataset(**kwargs)\n",
    "    \n",
    "    kwargs = {'data': val_dataset, 'labels': labels[int(len(labels)*0.5):int(len(labels)*0.7)]}\n",
    "    val_dataset = GEDataset(**kwargs)\n",
    "    \n",
    "    kwargs = {'data': test_dataset, 'labels': labels[int(len(labels)*0.7):]}\n",
    "    test_dataset = GEDataset(**kwargs)\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "def normalize_data(column):\n",
    "    mean = np.mean(column)\n",
    "    std = np.std(column)\n",
    "    if std == 0.0:\n",
    "        return column\n",
    "    return (column - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network (5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement your two hidden layer neural network. GENet class will represent your neural network. First hidden layer will contain 64 neurons and second hidden layer will contain 32 neurons. You will decide the number of input and output neurons.  Use ReLU as your hidden activation functions. You need to pick a proper activation function for the output layer. Implement the functions with missing bodies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GENet(nn.Module):\n",
    "    '''Define your neural network'''\n",
    "    def __init__(self, **kwargs): # you can add any additional parameters you want \n",
    "    # You should create your neural network here\n",
    "        super(GENet, self).__init__()\n",
    "        self.hidden1 = nn.Sequential(\n",
    "            nn.Linear(20531, 64),\n",
    "            nn.ReLU())\n",
    "        self.hidden2 = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU())\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(32, 5))\n",
    "     \n",
    "    def forward(self, X): # you can add any additional parameters you want\n",
    "    # Forward propagation implementation should be here\n",
    "        out = self.hidden1(X)\n",
    "        out = self.hidden2(out)\n",
    "        out = self.output(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training (7 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete below code snippet to train your network. You need to carefully select the appropriate loss function. You need to select appropriate values for hyper-parameters. You need to use SGD optimizer for this question. So far, you should have created three dataset splits for training, validation and testing above. Note that you will not do cross validation. You will need to load these splits at this phase. Make sure that you shuffle the samples in the training split. Plot training loss and training accuracy of each iteration (each batch). Also plot validation loss and accuracy at each epoch. Use matplotlib library for plotting. Your model is going to run upto the max epoch parameter. Pick the best model so far as your resulting model. You need to save this model in a \".pth\" file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "#HINT: note that your training time should not take many days.\n",
    "\n",
    "#TODO:\n",
    "#Pick your hyper parameters\n",
    "max_epoch = 8\n",
    "train_batch = 64\n",
    "val_batch = 32\n",
    "test_batch = 48\n",
    "learning_rate = 0.001\n",
    "\n",
    "#use_gpu = torch.cuda.is_available()\n",
    "\n",
    "# Inspired from: https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = get_dataset(\"./\")\n",
    "def main(train_results, val_results): # you are free to change parameters\n",
    "    \n",
    "    # Create train dataset loader\n",
    "    trainloader = DataLoader(train_dataset, batch_size=train_batch, shuffle=True)\n",
    "    \n",
    "    # Create validation dataset loader\n",
    "    val_loader = DataLoader(val_dataset, batch_size=val_batch)\n",
    "    \n",
    "    # initialize your GENet neural network\n",
    "    model = GENet()\n",
    "    \n",
    "    # define your loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(list(model.parameters()), lr=learning_rate, momentum=0.9, weight_decay=5e-04) # you can play with momentum and weight_decay parameters as well\n",
    "    \n",
    "    # start training\n",
    "    # for each epoch calculate validation performance\n",
    "    # save best model according to validation performance\n",
    "    \n",
    "    best_acc = 0\n",
    "    best_path = \"best_model.pth\"\n",
    "    for epoch in range(max_epoch):\n",
    "        res = train(epoch, model, criterion, optimizer, trainloader)\n",
    "        acc = res[0]\n",
    "        train_results.append(res)\n",
    "        res = test(model, val_loader, criterion)\n",
    "        val_results.append(res)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "\n",
    "''' Train your network for a one epoch '''\n",
    "def train(epoch, model, criterion, optimizer, loader): # you are free to change parameters\n",
    "    model.train()\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    accuracies = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    t1 = time.time()\n",
    "    for batch_idx, (data, labels) in enumerate(loader):\n",
    "        data_time.update(time.time() - t1)\n",
    "        \n",
    "        # Implement training code for a one iteration\n",
    "        \n",
    "        output = model(data.float())\n",
    "        loss = criterion(output, labels)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        acc = sum([1 if item[0] == item[1] else 0 for item in zip(predicted, labels)]) / len(data)\n",
    "        accuracies.update(acc, data.size(0))\n",
    "        losses.update(loss.item(), data.size(0))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_time.update(time.time() - t1)\n",
    "        t1 = time.time()\n",
    "        \n",
    "        print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "              'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "              'Data {data_time.val:.4f} ({data_time.avg:.4f})\\t'\n",
    "              'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "              'Accu {acc.val:.4f} ({acc.avg:.4f})\\t'.format(\n",
    "               epoch + 1, batch_idx + 1, len(loader), \n",
    "               batch_time=batch_time,\n",
    "               data_time=data_time, \n",
    "               loss=losses,\n",
    "               acc=accuracies))\n",
    "    \n",
    "    return accuracies.avg, losses.avg\n",
    "\n",
    "\n",
    "''' Test&Validate your network '''\n",
    "def test(model, loader, criterion): # you are free to change parameters\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    accuracies = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        t1 = time.time()\n",
    "        for batch_idx, (data, labels) in enumerate(loader):\n",
    "            # Implement test code\n",
    "            output = model(data.float())\n",
    "            loss = criterion(output, labels)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            acc = sum([1 if item[0] == item[1] else 0 for item in zip(predicted, labels)]) / len(data)\n",
    "            accuracies.update(acc, data.size(0))\n",
    "            losses.update(loss.item(), data.size(0))\n",
    "            \n",
    "            batch_time.update(time.time() - t1)\n",
    "            t1 = time.time()\n",
    "            \n",
    "        print('Time {batch_time.avg:.3f}\\t'\n",
    "              'Accu {acc.avg:.4f}\\t'\n",
    "              'Loss {loss.avg:.4f}\\t'.format(\n",
    "               batch_time=batch_time, \n",
    "               acc=accuracies,\n",
    "               loss=losses))\n",
    "        \n",
    "        return accuracies.avg, losses.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Your Results [4 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For us to assess if your model has learnt as expected, you have to provide training loss, training accuracy, validation loss and validation accuracy plots. You need to provide two distinct plots, one demonstrating training and validation loss scores and the other demonstrating training and validation accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][1/7]\tTime 0.106 (0.106)\tData 0.0358 (0.0358)\tLoss 1.6297 (1.6297)\tAccu 0.2188 (0.2188)\t\n",
      "Epoch: [1][2/7]\tTime 0.033 (0.070)\tData 0.0197 (0.0278)\tLoss 1.6201 (1.6249)\tAccu 0.2344 (0.2266)\t\n",
      "Epoch: [1][3/7]\tTime 0.030 (0.056)\tData 0.0188 (0.0248)\tLoss 1.5806 (1.6101)\tAccu 0.3906 (0.2812)\t\n",
      "Epoch: [1][4/7]\tTime 0.029 (0.050)\tData 0.0179 (0.0231)\tLoss 1.5667 (1.5993)\tAccu 0.3906 (0.3086)\t\n",
      "Epoch: [1][5/7]\tTime 0.026 (0.045)\tData 0.0165 (0.0217)\tLoss 1.5079 (1.5810)\tAccu 0.5625 (0.3594)\t\n",
      "Epoch: [1][6/7]\tTime 0.027 (0.042)\tData 0.0176 (0.0211)\tLoss 1.5109 (1.5693)\tAccu 0.4688 (0.3776)\t\n",
      "Epoch: [1][7/7]\tTime 0.014 (0.038)\tData 0.0080 (0.0192)\tLoss 1.4818 (1.5658)\tAccu 0.5625 (0.3850)\t\n",
      "Time 0.008\tAccu 0.6750\tLoss 1.3986\t\n",
      "Epoch: [2][1/7]\tTime 0.030 (0.030)\tData 0.0195 (0.0195)\tLoss 1.3827 (1.3827)\tAccu 0.7344 (0.7344)\t\n",
      "Epoch: [2][2/7]\tTime 0.027 (0.029)\tData 0.0181 (0.0188)\tLoss 1.3089 (1.3458)\tAccu 0.7812 (0.7578)\t\n",
      "Epoch: [2][3/7]\tTime 0.029 (0.029)\tData 0.0199 (0.0192)\tLoss 1.2611 (1.3175)\tAccu 0.8125 (0.7760)\t\n",
      "Epoch: [2][4/7]\tTime 0.029 (0.029)\tData 0.0199 (0.0194)\tLoss 1.2101 (1.2907)\tAccu 0.8438 (0.7930)\t\n",
      "Epoch: [2][5/7]\tTime 0.030 (0.029)\tData 0.0197 (0.0194)\tLoss 1.1553 (1.2636)\tAccu 0.9219 (0.8187)\t\n",
      "Epoch: [2][6/7]\tTime 0.029 (0.029)\tData 0.0198 (0.0195)\tLoss 1.1713 (1.2482)\tAccu 0.9531 (0.8411)\t\n",
      "Epoch: [2][7/7]\tTime 0.012 (0.027)\tData 0.0068 (0.0177)\tLoss 1.0444 (1.2401)\tAccu 1.0000 (0.8475)\t\n",
      "Time 0.009\tAccu 0.9500\tLoss 1.0089\t\n",
      "Epoch: [3][1/7]\tTime 0.033 (0.033)\tData 0.0223 (0.0223)\tLoss 0.9569 (0.9569)\tAccu 0.9844 (0.9844)\t\n",
      "Epoch: [3][2/7]\tTime 0.030 (0.031)\tData 0.0201 (0.0212)\tLoss 1.0038 (0.9803)\tAccu 0.9688 (0.9766)\t\n",
      "Epoch: [3][3/7]\tTime 0.030 (0.031)\tData 0.0202 (0.0209)\tLoss 0.8961 (0.9523)\tAccu 0.9531 (0.9688)\t\n",
      "Epoch: [3][4/7]\tTime 0.029 (0.031)\tData 0.0194 (0.0205)\tLoss 0.7779 (0.9087)\tAccu 0.9844 (0.9727)\t\n",
      "Epoch: [3][5/7]\tTime 0.030 (0.030)\tData 0.0196 (0.0203)\tLoss 0.7862 (0.8842)\tAccu 1.0000 (0.9781)\t\n",
      "Epoch: [3][6/7]\tTime 0.027 (0.030)\tData 0.0166 (0.0197)\tLoss 0.7047 (0.8543)\tAccu 0.9844 (0.9792)\t\n",
      "Epoch: [3][7/7]\tTime 0.013 (0.027)\tData 0.0069 (0.0179)\tLoss 0.6542 (0.8463)\tAccu 1.0000 (0.9800)\t\n",
      "Time 0.009\tAccu 0.9812\tLoss 0.6524\t\n",
      "Epoch: [4][1/7]\tTime 0.032 (0.032)\tData 0.0217 (0.0217)\tLoss 0.7445 (0.7445)\tAccu 0.9844 (0.9844)\t\n",
      "Epoch: [4][2/7]\tTime 0.031 (0.031)\tData 0.0205 (0.0211)\tLoss 0.5532 (0.6488)\tAccu 1.0000 (0.9922)\t\n",
      "Epoch: [4][3/7]\tTime 0.028 (0.030)\tData 0.0184 (0.0202)\tLoss 0.5448 (0.6142)\tAccu 1.0000 (0.9948)\t\n",
      "Epoch: [4][4/7]\tTime 0.029 (0.030)\tData 0.0200 (0.0201)\tLoss 0.5380 (0.5951)\tAccu 0.9844 (0.9922)\t\n",
      "Epoch: [4][5/7]\tTime 0.028 (0.030)\tData 0.0183 (0.0198)\tLoss 0.4001 (0.5561)\tAccu 1.0000 (0.9938)\t\n",
      "Epoch: [4][6/7]\tTime 0.027 (0.029)\tData 0.0172 (0.0193)\tLoss 0.4271 (0.5346)\tAccu 1.0000 (0.9948)\t\n",
      "Epoch: [4][7/7]\tTime 0.012 (0.027)\tData 0.0059 (0.0174)\tLoss 0.4760 (0.5323)\tAccu 1.0000 (0.9950)\t\n",
      "Time 0.009\tAccu 0.9938\tLoss 0.3947\t\n",
      "Epoch: [5][1/7]\tTime 0.031 (0.031)\tData 0.0210 (0.0210)\tLoss 0.4258 (0.4258)\tAccu 1.0000 (1.0000)\t\n",
      "Epoch: [5][2/7]\tTime 0.030 (0.031)\tData 0.0201 (0.0205)\tLoss 0.3626 (0.3942)\tAccu 1.0000 (1.0000)\t\n",
      "Epoch: [5][3/7]\tTime 0.030 (0.030)\tData 0.0202 (0.0204)\tLoss 0.2736 (0.3540)\tAccu 1.0000 (1.0000)\t\n",
      "Epoch: [5][4/7]\tTime 0.027 (0.029)\tData 0.0174 (0.0197)\tLoss 0.3256 (0.3469)\tAccu 1.0000 (1.0000)\t\n",
      "Epoch: [5][5/7]\tTime 0.030 (0.029)\tData 0.0201 (0.0198)\tLoss 0.2635 (0.3302)\tAccu 0.9688 (0.9938)\t\n",
      "Epoch: [5][6/7]\tTime 0.028 (0.029)\tData 0.0183 (0.0195)\tLoss 0.2404 (0.3153)\tAccu 1.0000 (0.9948)\t\n",
      "Epoch: [5][7/7]\tTime 0.012 (0.027)\tData 0.0070 (0.0177)\tLoss 0.1640 (0.3092)\tAccu 1.0000 (0.9950)\t\n",
      "Time 0.009\tAccu 1.0000\tLoss 0.2125\t\n",
      "Epoch: [6][1/7]\tTime 0.033 (0.033)\tData 0.0229 (0.0229)\tLoss 0.1772 (0.1772)\tAccu 1.0000 (1.0000)\t\n",
      "Epoch: [6][2/7]\tTime 0.028 (0.030)\tData 0.0188 (0.0209)\tLoss 0.1780 (0.1776)\tAccu 1.0000 (1.0000)\t\n",
      "Epoch: [6][3/7]\tTime 0.031 (0.031)\tData 0.0206 (0.0208)\tLoss 0.1887 (0.1813)\tAccu 1.0000 (1.0000)\t\n",
      "Epoch: [6][4/7]\tTime 0.028 (0.030)\tData 0.0185 (0.0202)\tLoss 0.1730 (0.1792)\tAccu 1.0000 (1.0000)\t\n",
      "Epoch: [6][5/7]\tTime 0.032 (0.030)\tData 0.0215 (0.0205)\tLoss 0.1297 (0.1693)\tAccu 1.0000 (1.0000)\t\n",
      "Epoch: [6][6/7]\tTime 0.033 (0.031)\tData 0.0210 (0.0206)\tLoss 0.1265 (0.1622)\tAccu 0.9844 (0.9974)\t\n",
      "Epoch: [6][7/7]\tTime 0.010 (0.028)\tData 0.0052 (0.0184)\tLoss 0.0734 (0.1586)\tAccu 1.0000 (0.9975)\t\n",
      "Time 0.009\tAccu 1.0000\tLoss 0.1136\t\n",
      "Epoch: [7][1/7]\tTime 0.033 (0.033)\tData 0.0226 (0.0226)\tLoss 0.1174 (0.1174)\tAccu 1.0000 (1.0000)\t\n",
      "Epoch: [7][2/7]\tTime 0.030 (0.031)\tData 0.0189 (0.0208)\tLoss 0.0778 (0.0976)\tAccu 1.0000 (1.0000)\t\n",
      "Epoch: [7][3/7]\tTime 0.032 (0.032)\tData 0.0212 (0.0209)\tLoss 0.0709 (0.0887)\tAccu 1.0000 (1.0000)\t\n",
      "Epoch: [7][4/7]\tTime 0.027 (0.030)\tData 0.0176 (0.0201)\tLoss 0.0687 (0.0837)\tAccu 1.0000 (1.0000)\t\n",
      "Epoch: [7][5/7]\tTime 0.027 (0.030)\tData 0.0179 (0.0197)\tLoss 0.0800 (0.0830)\tAccu 1.0000 (1.0000)\t\n",
      "Epoch: [7][6/7]\tTime 0.029 (0.030)\tData 0.0201 (0.0197)\tLoss 0.0581 (0.0788)\tAccu 1.0000 (1.0000)\t\n",
      "Epoch: [7][7/7]\tTime 0.010 (0.027)\tData 0.0049 (0.0176)\tLoss 0.1816 (0.0829)\tAccu 0.9375 (0.9975)\t\n",
      "Time 0.010\tAccu 1.0000\tLoss 0.0694\t\n",
      "Epoch: [8][1/7]\tTime 0.027 (0.027)\tData 0.0161 (0.0161)\tLoss 0.0453 (0.0453)\tAccu 1.0000 (1.0000)\t\n",
      "Epoch: [8][2/7]\tTime 0.030 (0.028)\tData 0.0201 (0.0181)\tLoss 0.0491 (0.0472)\tAccu 1.0000 (1.0000)\t\n",
      "Epoch: [8][3/7]\tTime 0.029 (0.029)\tData 0.0197 (0.0186)\tLoss 0.0596 (0.0513)\tAccu 1.0000 (1.0000)\t\n",
      "Epoch: [8][4/7]\tTime 0.028 (0.029)\tData 0.0183 (0.0186)\tLoss 0.0604 (0.0536)\tAccu 0.9844 (0.9961)\t\n",
      "Epoch: [8][5/7]\tTime 0.028 (0.029)\tData 0.0190 (0.0186)\tLoss 0.0532 (0.0535)\tAccu 1.0000 (0.9969)\t\n",
      "Epoch: [8][6/7]\tTime 0.028 (0.028)\tData 0.0182 (0.0186)\tLoss 0.0342 (0.0503)\tAccu 1.0000 (0.9974)\t\n",
      "Epoch: [8][7/7]\tTime 0.012 (0.026)\tData 0.0068 (0.0169)\tLoss 0.0435 (0.0500)\tAccu 1.0000 (0.9975)\t\n",
      "Time 0.009\tAccu 1.0000\tLoss 0.0482\t\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# write your code in this cell to plot your results\n",
    "train_res = list()\n",
    "val_res = list()\n",
    "main(train_res, val_res)\n",
    "\n",
    "train_acc, train_loss = zip(*train_res)\n",
    "val_acc, val_loss = zip(*val_res)\n",
    "\n",
    "plt.plot(range(1, max_epoch+1), train_acc, label=\"train_acc\")\n",
    "plt.plot(range(1, max_epoch+1), val_acc, label=\"val_acc\")\n",
    "plt.title(\"Epoch - Accuracy Graph\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(1, max_epoch+1), train_loss, label=\"train_loss\")\n",
    "plt.plot(range(1, max_epoch+1), val_loss, label=\"val_loss\")\n",
    "plt.title(\"Epoch - Loss Graph\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing [7 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will test your final model with test dataset in this section. You should provide confusion matrix as deliverable. Report confusion matrix in your pdf file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 0.011\tAccu 0.9917\t\n",
      "\n",
      "\t\t\tActual\n",
      "\n",
      "\tBRCA\tKIRC\tCOAD\tLUAD\tPRAD\t\n",
      "BRCA\t 92\t 0\t 0\t 0\t 0\t\n",
      "KIRC\t 0\t 42\t 0\t 0\t 0\t\n",
      "COAD\t 0\t 0\t 23\t 0\t 0\tPredicted\n",
      "LUAD\t 1\t 0\t 1\t 42\t 0\t\n",
      "PRAD\t 0\t 0\t 0\t 0\t 40\t\n"
     ]
    }
   ],
   "source": [
    "# write your code in this cell to test your best model with the test dataset\n",
    "# Create test dataset loader\n",
    "testloader = DataLoader(test_dataset, batch_size=test_batch)\n",
    "\n",
    "best_path = \"best_model.pth\"\n",
    "model = GENet()\n",
    "checkpoint = torch.load(best_path)\n",
    "if isinstance(checkpoint, dict) and 'state_dict' in checkpoint:\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "else:\n",
    "    model.load_state_dict(checkpoint)\n",
    "    \n",
    "batch_time = AverageMeter()\n",
    "accuracies = AverageMeter()\n",
    "\n",
    "labels_list = list()\n",
    "predictions_list = list()\n",
    "\n",
    "with torch.no_grad():\n",
    "    t1 = time.time()\n",
    "    for batch_idx, (data, labels) in enumerate(testloader):\n",
    "        # Implement test code\n",
    "        output = model(data.float())\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        labels_list.append(list(np.asarray(labels)))\n",
    "        predictions_list.append(list(np.asarray(predicted)))\n",
    "        acc = sum([1 if item[0] == item[1] else 0 for item in zip(predicted, labels)]) / len(data)\n",
    "        accuracies.update(acc, data.size(0))\n",
    "\n",
    "        batch_time.update(time.time() - t1)\n",
    "        t1 = time.time()\n",
    "\n",
    "    print('Time {batch_time.avg:.3f}\\t'\n",
    "          'Accu {acc.avg:.4f}\\t'.format(\n",
    "           batch_time=batch_time, \n",
    "           acc=accuracies))\n",
    "\n",
    "confusion_matrix = np.asarray([[0,0,0,0,0] for i in range(0,5)])\n",
    "predictions_list = [item for sublist in predictions_list for item in sublist]\n",
    "labels_list = [item for sublist in labels_list for item in sublist]\n",
    "\n",
    "for item in range(len(predictions_list)):\n",
    "    i = predictions_list[item]\n",
    "    j = labels_list[item]\n",
    "    confusion_matrix[i][j] += 1\n",
    "\n",
    "print()\n",
    "print(\"\\t\\t\\tActual\\n\")\n",
    "row_str = \"\\t\"\n",
    "for index in index_to_label:\n",
    "    row_str += index_to_label[index] + \"\\t\"\n",
    "print(row_str)\n",
    "\n",
    "for i, row in enumerate(confusion_matrix):\n",
    "    row_str = index_to_label[i] + \"\\t\"\n",
    "    for item in row:\n",
    "        row_str += \" \" + str(item) + \"\\t\"\n",
    "    if i == 2:\n",
    "        row_str += \"Predicted\"\n",
    "    print(row_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 [53 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, you will train a neural networks to recognize house numbers from images. You will use the dataset at the following link: http://ufldl.stanford.edu/housenumbers/. You are actually asked to predict the number in the middle (i.e., for 173, predict 7). Images are cropped accordingly and correct labels are given in the dataset. You do not need to worry about other numbers that might still be visible in the image. You have to use <b>train_32x32.mat</b> and <b>test_32x32.mat</b> from <b>the CROPPED version (FORMAT2)</b>. All images in this dataset have the following dimensions: 32x32x3. You will solve this problem using two architectures: (1) Using a MLP and (2) Using a CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Multi Layer Perceptron (MLP) [23 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataLoader [3 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, you will train an MLP. Here, you are supposed to flatten the image into a vector (also to grayscale). Note that the pixel values also needs to be normalized to [0,1] range. First, implement the data loader (SVHNDataset) as you did in Question 2. For this question, do not perform cross validation also do not use validation split. Note that training and test splits are given to you in the provided link. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "#http://ufldl.stanford.edu/housenumbers/\n",
    "class SVHNDataset(Dataset):\n",
    "    # Define constructor for SVHNDataset class\n",
    "    # HINT: You can pass processed data samples and their ground truth values as parameters \n",
    "    def __init__(self, data, labels): # you are free to change parameters\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        \n",
    "    '''This function should return sample count in the dataset'''\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    '''This function should return a single sample and its ground truth value from the dataset corresponding to index parameter '''\n",
    "    def __getitem__(self, index):\n",
    "        _x = data[index]\n",
    "        _y = labels[index]\n",
    "        return _x, _y\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    r = rgb[:,:,0]\n",
    "    g = rgb[:,:,1]\n",
    "    b = rgb[:,:,2]\n",
    "    return r*0.299 + g*0.587 + b*0.114\n",
    "\n",
    "def normalize_array(img):\n",
    "    minn = min(img)\n",
    "    maxx = max(img)\n",
    "    img = np.asarray([(x - minn)/(maxx - minn) for x in img])\n",
    "    return img\n",
    "\n",
    "def process_image(img):\n",
    "    img = rgb2gray(img)\n",
    "    img = img.flatten()\n",
    "    img = normalize_array(img)\n",
    "    return img\n",
    "def get_dataset(root): #you are free to change parameters\n",
    "    # Read dataset files from \"data/Q2\" folder\n",
    "    # Normalize datasets\n",
    "    train = sio.loadmat(root + \"data/Q2/train_32x32.mat\")\n",
    "    train_label = train['y'].T[0]\n",
    "    \n",
    "    train_data = train['X']\n",
    "    processed_train_data = list()\n",
    "    for i in range(train_data.shape[3]):\n",
    "        img = train_data[:,:,:,i]\n",
    "        img = process_image(img)\n",
    "        processed_train_data.append(img)\n",
    "    \n",
    "    train_data = np.asarray(processed_train_data)\n",
    "    train_dataset = SVHNDataset(train_data, train_label)\n",
    "    \n",
    "    test = sio.loadmat(root + \"data/Q2/test_32x32.mat\")\n",
    "    test_label = test['y'].T[0]\n",
    "    \n",
    "    test_data = test['X']\n",
    "    processed_test_data = list()\n",
    "    for i in range(test_data.shape[3]):\n",
    "        img = test_data[:,:,:,i]\n",
    "        img = process_image(img)\n",
    "        processed_test_data.append(img)\n",
    "    \n",
    "    test_data = np.asarray(processed_test_data)\n",
    "    test_dataset = SVHNDataset(test_data, test_label)\n",
    "    \n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network [4 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement your two hidden layered neural network. FCNet class will represent your neural network. First hidden layer will contain 256 neurons and second hidden layer will contain 256 neurons. You will decide the number of input and output neurons.  Use ReLU as your hidden activation functions. You need to pick a proper activation function for the output layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNet(nn.Module):\n",
    "    '''Define your neural network'''\n",
    "    def __init__(self, **kwargs): # you can add any additional parameters you want \n",
    "    # You should create your neural network here\n",
    "        super(FCNet, self).__init__()\n",
    "        self.hidden1 = nn.Sequential(\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU())\n",
    "        self.hidden2 = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU())\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(256, 10))\n",
    "     \n",
    "    def forward(self, X): # you can add any additional parameters you want\n",
    "    # Forward propagation implementation should be here\n",
    "        out = self.hidden1(X)\n",
    "        out = self.hidden2(out)\n",
    "        out = self.output(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Testing [10 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, train your network. You need to select the appropriate loss function. You need to select appropriate hyper-parameters' values. Make sure that you shuffle the samples in the training split. Note that you will not do cross validation. Plot the training loss and accuracy for each iteration. Plot the test loss and accuracy for each epoch. Your model is going to run upto the max epoch parameter. Pick the best model as your resulting model. You need to save this model in a \".pth\" file. (HINT: note that your training time should not take many days.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# TODO:\n",
    "# Pick your hyper parameters\n",
    "# max_epoch = \n",
    "# train_batch = \n",
    "# test_batch = \n",
    "# learning_rate =\n",
    "\n",
    "#use_gpu = torch.cuda.is_available()\n",
    "\n",
    "\n",
    "def main(): # you are free to change parameters\n",
    "\n",
    "    # Create train dataset loader\n",
    "    # Create validation dataset loader\n",
    "    # Create test dataset loader\n",
    "    # initialize your GENet neural network\n",
    "    # define your loss function\n",
    "    \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-04) # you can play with momentum and weight_decay parameters as well\n",
    "    \n",
    "    # start training\n",
    "    # for each epoch calculate validation performance\n",
    "    # save best model according to validation performance\n",
    "    \n",
    "    #for epoch in range(max_epoch):\n",
    "    #    train(epoch, model, criterion, optimizer, trainloader)\n",
    "    #    acc = test(model, val_loader)\n",
    "    #    if acc > best_acc:\n",
    "    #       torch.save(model, best_path)\n",
    "    \n",
    "''' Train your network for a one epoch '''\n",
    "def train(epoch, model, criterion, optimizer, loader): # you are free to change parameters\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (data, labels) in enumerate(loader):\n",
    "        # TODO:\n",
    "        # Implement training code for a one iteration\n",
    "        \n",
    "        print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "              'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "              'Data {data_time.val:.4f} ({data_time.avg:.4f})\\t'\n",
    "              'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "              'Accu {acc.val:.4f} ({acc.avg:.4f})\\t'.format(\n",
    "               epoch + 1, batch_idx + 1, len(trainloader), \n",
    "               batch_time=batch_time,\n",
    "               data_time=data_time, \n",
    "               loss=losses,\n",
    "               acc=accuracies))\n",
    "\n",
    "\n",
    "''' Test&Validate your network '''\n",
    "def test(model, loader): # you are free to change parameters\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, labels) in enumerate(testloader):\n",
    "            # TODO:\n",
    "            # Implement test code\n",
    "            \n",
    "        print('Time {batch_time.avg:.3f}\\t'\n",
    "              'Accu {acc.avg:.4f}\\t'.format(\n",
    "               batch_time=batch_time, \n",
    "               acc=accuracies))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weight Visualization [6 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the best model, extract the learned weights from first hidden layer. Visualize learned weights for each neuron of the first hidden layer as an image. Show these images in a grid and add this grid in the pdf file as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code in this cell to visualize first hidden layer weights. Produce your figure here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Convolutional Neural Network (CNN) [30 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataLoader [3 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, you will train a CNN for the same problem. Again, the pixel values also needs to be normalized to [0,1] range. First, implement the data loader (SVHNDataset). Note that now you do not need to flatten the image. Again, for this question, do not perform cross validation. Also, just like 3.1., do not use a validation split. Use the same splits as in 3.1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://ufldl.stanford.edu/housenumbers/\n",
    "class SVHNDataset(Dataset):\n",
    "    # TODO:\n",
    "    # Define constructor for SVHNDataset class\n",
    "    # HINT: You can pass processed data samples and their ground truth values as parameters \n",
    "    #def __init__(self, **kwargs): # you are free to change parameters\n",
    "        \n",
    "    '''This function should return sample count in the dataset'''\n",
    "    #def __len__(self):\n",
    "    #    return self.data.shape[0]\n",
    "\n",
    "    '''This function should return a single sample and its ground truth value from the dataset corresponding to index parameter '''\n",
    "    #def __getitem__(self, index):\n",
    "        #return _x, _y\n",
    "\n",
    "        \n",
    "def get_dataset(root): # you are free to change parameters\n",
    "    # TODO: \n",
    "    # Read dataset files from \"data/Q2\" folder\n",
    "    # Normalize datasets\n",
    "    \n",
    "    #return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network [9 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement your CNN. ConvNet class will represent your convolutional neural network. Implement 3 layers of convolution: \n",
    "<ul>\n",
    "    <li>(1) 32 filters with size of 3 x 3 with stride 1 and no padding, (2) ReLU </li>\n",
    "    <li>(3) 64 filters with size of 3 x 3 with stride 1 and no padding, (4) ReLU and (5) MaxPool 2 x 2 </li>\n",
    "    <li>(6) 128 filters with size of 3 x 3 with stride 1 and no padding, (7) ReLU and (8) MaxPool 2 x 2 </li> \n",
    "</ul>\n",
    "\n",
    "As a classifier layer, you need to add only one linear layer at the end of the network. You need to choose the appropriate input and output neuron sizes for the classification (linear) layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    '''Define your neural network'''\n",
    "    def __init__(self, **kwargs): # you can add any additional parameters you want \n",
    "    # TODO:\n",
    "    # You should create your neural network here\n",
    "     \n",
    "    def forward(self, X): # you can add any additional parameters you want\n",
    "    # TODO:\n",
    "    # Forward propagation implementation should be here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Testing [10 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, train your network. You need to select the appropriate loss function. You need to select your hyper-parameters. Make sure that you shuffle the samples in the training split. Note that you will not do cross validation. Plot the training loss and accuracy for each iteration. Plot the test loss and accuracy for each epoch. Your model is going to run upto the max epoch parameter. Pick the best model as your resulting model. You need to save this model in a \".pth\" file. Report the validation performance change between MLP and CONV neural network and explain the reason for this change. You need to add this explanation and your plots into the pdf file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# HINT: note that your training time should not take many days.\n",
    "\n",
    "# TODO:\n",
    "# Pick your hyper parameters\n",
    "# max_epoch = \n",
    "# train_batch = \n",
    "# test_batch = \n",
    "# learning_rate =\n",
    "\n",
    "#use_gpu = torch.cuda.is_available()\n",
    "\n",
    "\n",
    "def main(): # you are free to change parameters\n",
    "\n",
    "    # Create train dataset loader\n",
    "    # Create validation dataset loader\n",
    "    # Create test dataset loader\n",
    "    # initialize your GENet neural network\n",
    "    # define your loss function\n",
    "    \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-04) # you can play with momentum and weight_decay parameters as well\n",
    "    \n",
    "    # start training\n",
    "    # for each epoch calculate validation performance\n",
    "    # save best model according to validation performance\n",
    "    \n",
    "    #for epoch in range(max_epoch):\n",
    "    #    train(epoch, model, criterion, optimizer, trainloader)\n",
    "    #    acc = test(model, val_loader)\n",
    "    #    if acc > best_acc:\n",
    "    #       torch.save(model, best_path)\n",
    "    \n",
    "''' Train your network for a one epoch '''\n",
    "def train(epoch, model, criterion, optimizer, loader): # you are free to change parameters\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (data, labels) in enumerate(loader):\n",
    "        # TODO:\n",
    "        # Implement training code for a one iteration\n",
    "        \n",
    "        print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "              'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "              'Data {data_time.val:.4f} ({data_time.avg:.4f})\\t'\n",
    "              'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "              'Accu {acc.val:.4f} ({acc.avg:.4f})\\t'.format(\n",
    "               epoch + 1, batch_idx + 1, len(trainloader), \n",
    "               batch_time=batch_time,\n",
    "               data_time=data_time, \n",
    "               loss=losses,\n",
    "               acc=accuracies))\n",
    "\n",
    "\n",
    "''' Test&Validate your network '''\n",
    "def test(model, loader): # you are free to change parameters\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, labels) in enumerate(testloader):\n",
    "            # TODO:\n",
    "            # Implement test code\n",
    "            \n",
    "        print('Time {batch_time.avg:.3f}\\t'\n",
    "              'Accu {acc.avg:.4f}\\t'.format(\n",
    "               batch_time=batch_time, \n",
    "               acc=accuracies))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weight Visualization [8 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the best convolutional model you obtained, extract the learnt weights from the first convolutional layer. Visualize each learnt filter of the first convolutional layer as an image. Show these images in a grid and add this grid in to the pdf file as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code in this cell to visualize filters of the first convolutional layer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BONUS: Question 4 [20 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learnt weights of a network that has been trained to solve a problem can be used as a starting point for the weights of neural networks that will be used to solve another similar problem. Instead of random initialization for the weights of the neural networks, using the weights of the neural network trained to solve a similar problem will make learning easier. This approach is called <b>\"Transfer Learning\"</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this question, you will use CIFAR10 dataset. You can find detailed information about the dataset from https://www.cs.toronto.edu/~kriz/cifar.html. Torchvision library provides a ready-to-use data loader for the CIFAR10 dataset. You <b>DO NOT</b> need to implement your custom data loader for this question. You can easily split the dataset into training and test. Just set the \"train\" parameter of the constructor of torchvision.datasets.CIFAR10 class.  You will the use test split as the validation set at each epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have to use AlexNet convolutional neural network which is one of the most basic CNN models. You <b>DO NOT</b> need to implement your own AlexNet architecture. Torchvision has also a model zoo which contains commonly used CNN models including AlexNet. Therefore, you need to use the AlexNet model from the Torchvision library. For the question 4.2 Transfer Learning part, you will need to transfer pretrained network weights as a starting point. These weights will come from the result of the training with Imagenet dataset. These will be loaded automatically when you set the \"pretrained\" parameter to true (check the hints in the code). Otherwise, weights will be randomly initialized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Random Initialization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 Train & Validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have to train randomly initialized AlexNet CNN model using CIFAR10 dataset. At the end of each epoch, you should evaluate your network with the test split. Print (not plot) training loss, training accuracy, validation loss and validation accuracy values for each epoch. When the training is completed, print the best validation score that you have obtained during training. Report this score in your pdf file as well. You will compare this score with the result of the next question. Note that this might take a long time because of randomly initilized weights. Stop training when validation score converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code in this cell to train your network that has randomly selected weights\n",
    "\n",
    "# HINTS:\n",
    "# alexnet = torchvision.models.alexnet(pretrained=False) \n",
    "# torchvision.models.alexnet(pretrained=False) will return an alexnet model instance with randomly initialized weights\n",
    "# you need to make some changes in the classifier layer to get a proper network for your problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 Kernel Output Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have trained AlexNet which has randomly initialized weights by using the CIFAR10 dataset. Now you have to plot the output of the each filter at the first convolution layer as an image by using randomly selected single validation image. Merge each image obtained from the corresponding kernel in a squared grid format. Add your plot in to the pdf file as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code in this cell to visualize output of the each filter at the first conv layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 Train & Validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, you have to train ImageNet pretrained AlexNet model for the CIFAR10 dataset. At the end of each epoch, you should evaluate your network with test split. Print training loss, training accuracy, validation loss and validation accuracy values for each epoch as an output of below cell. Report the best validation accuracy score. Compare validation scores that are obtained from these two different training approaches. Add your explanations to the pdf file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code in this cell to train your network using transfer learning approach\n",
    "\n",
    "# HINTS:\n",
    "# alexnet = torchvision.models.alexnet(pretrained=True) \n",
    "# torchvision.models.alexnet(pretrained=True) will return an alexnet model instance with ImageNet pretrained network weights.\n",
    "# you need to make some changes in the classifier layer to get a proper network for your problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 Kernel Output Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have trained AlexNet which is pretrained with ImageNet dataset for the CIFAR10 dataset. For this network, you have to plot the output of each filter at the first convolution layer as an image by using the same image that has picked 4.1.3 section. Merge each image obtained from the corresponding kernel in a squared grid format. Explain what these outputs mean. Compare your plot with the obtained from 4.1.2. You need to add your discussions and plot into the pdf file as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code in this cell to visualize output of the each filter at the first conv layer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
